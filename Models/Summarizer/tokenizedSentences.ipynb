{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To process embeddings\n",
    "import tensorflow_hub as hub\n",
    "from langdetect import detect\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# To create recommendations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# To create sentence clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# To load saved embeddings\n",
    "import joblib\n",
    "\n",
    "# To match strings\n",
    "import re\n",
    "\n",
    "# To create webapp\n",
    "import psutil\n",
    "import streamlit as st\n",
    "st.set_page_config(page_title='3380 Books',\n",
    "                  layout=\"wide\",\n",
    "                  page_icon= ':books:')\n",
    "from streamlit import caching\n",
    "\n",
    "\n",
    "# To sort final recommendation list\n",
    "from collections import Counter\n",
    "\n",
    "def dataLoader(datapath, books_file, reviews_file, reviewsAll_file):\n",
    "    '''\n",
    "    Loads DataFrames with books and review information\n",
    "    '''\n",
    "    books = pd.read_csv(datapath + books_file).drop('Unnamed: 0', axis=1).fillna('')\n",
    "    reviews = pd.read_csv(datapath + reviews_file).drop('Unnamed: 0', axis=1)\n",
    "    reviewsAll = pd.read_csv(datapath + reviewsAll_file).drop('Unnamed: 0', axis=1)\n",
    "    return books, reviews, reviewsAll\n",
    "\n",
    "def loadEmbeddings():\n",
    "    '''\n",
    "    Loads pre-trained sentence and review arrays\n",
    "    '''\n",
    "    # Path to USE\n",
    "    embed = hub.load('/media/einhard/Seagate Expansion Drive/3380_data/data/tensorflow_hub/universal-sentence-encoder_4')\n",
    "\n",
    "    # Load pre-trained sentence arrays\n",
    "    ## Reviews array is a set of embeddings trained on review lengths of < 90 characters\n",
    "    reviews_array = joblib.load('/media/einhard/Seagate Expansion Drive/3380_data/data/Models/reviewEmbeddings.pkl')\n",
    "    ## Descriptions array is a set of embeddings trained on all book descriptions\n",
    "    descriptions_array = joblib.load('/media/einhard/Seagate Expansion Drive/3380_data/data/Models/descriptionEmbeddings.pkl')\n",
    "\n",
    "    return embed, reviews_array, descriptions_array\n",
    "\n",
    "def embedInputs(books_df, review_df, search_param, review_max_len, searchTitle=True):\n",
    "    '''\n",
    "    Converts input reviews into USE arrays. Returns vectorized reviews for the book that was\n",
    "    passed as bookTitle.\n",
    "\n",
    "    Args:\n",
    "        search_param = List of book titles or author names whose reviews we want to embed. For authors, see 'searchTitle' argument\n",
    "        books_df = DataFrame with book_id information\n",
    "        review_df = DataFrame with book_id and review text\n",
    "        searchTitle = If True, will search for book_id based on title of a book. If False, it will look for author names to find book_id.\n",
    "    '''\n",
    "    if searchTitle:\n",
    "        #Finds book_id from title of book\n",
    "        input_book_id = books_df[books_df.title.isin([search_param])].book_id.tolist()\n",
    "    else:\n",
    "        # Finds book_id from author name\n",
    "        input_book_id = books_df[books_df.name.isin([search_param])].book_id.tolist()\n",
    "\n",
    "    # Finds reviews for specified book\n",
    "    input_sentences = review_df[review_df.book_id.isin(input_book_id)].review_text\n",
    "\n",
    "    # Filters review length\n",
    "    input_sentences = input_sentences[input_sentences.str.len() <= review_max_len]\n",
    "\n",
    "    # Converts reviews into 512-dimensional arrays\n",
    "    input_vectors = embed(input_sentences)\n",
    "\n",
    "    # Returns reviews and vectorized reviews for a particular book\n",
    "    return input_sentences, input_vectors\n",
    "\n",
    "def getClusters(input_vectors, n_clusters):\n",
    "    '''\n",
    "    Creates KMeans instance and fits model.\n",
    "\n",
    "    The for nested for loop is used to display the returned sentences on Streamlit.\n",
    "\n",
    "    Args:\n",
    "        input_sentences =  Sentences to compare\n",
    "        n_clusters = How many clusters to generate\n",
    "    '''\n",
    "    tsne = TSNE(n_components=3, verbose=1, perplexity=80, n_iter=5000, learning_rate=200)\n",
    "    TSNE_transformed = tsne.fit_transform(input_vectors)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, algorithm='full')\n",
    "    return kmeans.fit(TSNE_transformed)\n",
    "\n",
    "\n",
    "def showClusters(input_sentences, input_vectors, authorTitle, n_clusters, n_results, model, searchTitle=True):\n",
    "    '''\n",
    "    This function will find theme clusters in the reviews of a particular book or set of sentences.\n",
    "    Uses cluster centers to find semantically similar sentences to the input vectors.\n",
    "\n",
    "    The nested for loop is used to display the returned sentences on Streamlit.\n",
    "\n",
    "    Args:\n",
    "        input_sentences =  Sentences to compare\n",
    "        input_vectors = USE Array generated by embedding input sentences\n",
    "        authorTitle = Title of book in question, or name of author --> Used to display header only\n",
    "        n_clusters = How many clusters to generate\n",
    "        n_results = How many sentences to display per n_cluster\n",
    "        model = The model used to create the clusters.\n",
    "    '''\n",
    "    if searchTitle:\n",
    "        # Displays which book's reviews are being clustered\n",
    "        print(f'Opinion clusters about *{authorTitle}*')\n",
    "    else:\n",
    "        print(f'Opinion clusters about {authorTitle}\\'s books')\n",
    "\n",
    "    # Iterates through centroids to and computes inner products to find nlargest\n",
    "    for i in range(n_clusters):\n",
    "        centre = model.cluster_centers_[i]\n",
    "        inner_product = np.inner(centre, input_vectors)\n",
    "        indices = pd.Series(inner_product).nlargest(n_results).index\n",
    "        clusteredInputs = list(input_sentences.iloc[indices])\n",
    "\n",
    "        # Prints reviews that are closest to centroid\n",
    "    \n",
    "        print(f'**Cluster #{i+1}**')\n",
    "        for sentence in clusteredInputs:\n",
    "            print(sentence)\n",
    "        return indices, inner_product\n",
    "\n",
    "def cleanAndTokenize(df, filepath, searchTitle, author):\n",
    "    if searchTitle:\n",
    "        if Path(filepath + 'app_data/book_id/' + df.book_id.iloc[1].astype(str) + '.csv').is_file():\n",
    "            sentences_df = pd.read_csv(filepath + 'app_data/book_id/' + df.book_id.iloc[1].astype(str) + '.csv').drop('Unnamed: 0', axis=1)\n",
    "        else:\n",
    "            reviews_df = clean_reviews(df)\n",
    "            sentences_df =  make_sentences(reviews_df)\n",
    "            sentences_df.to_csv(filepath + 'app_data/book_id/' + df.book_id.iloc[1].astype(str) + '.csv')\n",
    "        sentences_df.book_id = sentences_df.book_id.astype(int)\n",
    "        return sentences_df\n",
    "\n",
    "    else:\n",
    "        if Path(filepath + 'app_data/author/' + author + '.csv').is_file():\n",
    "            sentences_df = pd.read_csv(filepath + 'app_data/author/' + author + '.csv').drop('Unnamed: 0', axis=1)\n",
    "        else:\n",
    "            reviews_df = clean_reviews(df)\n",
    "            sentences_df =  make_sentences(reviews_df)\n",
    "            sentences_df.to_csv(filepath + 'app_data/author/' + author + '.csv')\n",
    "        sentences_df.book_id = sentences_df.book_id.astype(int)\n",
    "\n",
    "        return sentences_df\n",
    "def clean_reviews(df):\n",
    "    '''\n",
    "    Copyright (c) 2020 Willie Costello\n",
    "    '''\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Define spoiler marker & remove from all reviews\n",
    "    spoiler_str_ucsd = '\\*\\* spoiler alert \\*\\* \\n'\n",
    "    df['review_text'] = df['review_text'].str.replace(spoiler_str_ucsd, '')\n",
    "\n",
    "    # Replace all new line characters\n",
    "    df['review_text'] = df['review_text'].str.replace('\\n', ' ')\n",
    "\n",
    "    # Append space to all sentence end characters\n",
    "    df['review_text'] = df['review_text'].str.replace('.', '. ').replace('!', '! ').replace('?', '? ')\n",
    "\n",
    "    # Initialize dataframe to store English-language reviews\n",
    "    reviews_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each row in dataframe\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Save review to variable\n",
    "        review = df.iloc[i]['review_text']\n",
    "\n",
    "        # Check if review is English\n",
    "        if detect(review) == 'en':\n",
    "            # If so, add row to English-language dataframe\n",
    "            reviews_df = reviews_df.append(df.iloc[i, :])\n",
    "\n",
    "    reviews_df.book_id = reviews_df.book_id.astype(int)\n",
    "    return reviews_df\n",
    "\n",
    "\n",
    "def make_sentences(reviews_df):\n",
    "    '''\n",
    "    Copyright (c) 2020 Willie Costello\n",
    "    '''\n",
    "    # Initialize dataframe to store review sentences, and counter\n",
    "    sentences_df = pd.DataFrame()\n",
    "    ctr = 0\n",
    "\n",
    "\n",
    "    # Loop through each review\n",
    "    for i in range(len(reviews_df)):\n",
    "\n",
    "        # Save row and review to variables\n",
    "        row = reviews_df.iloc[i]\n",
    "        review = row.loc['review_text']\n",
    "\n",
    "        # Tokenize review into sentences\n",
    "        sentences = sent_tokenize(review)\n",
    "\n",
    "        # Loop through each sentence in list of tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            # Add row for sentence to sentences dataframe\n",
    "            new_row = row.copy()\n",
    "            new_row.at['review_text'] = sentence\n",
    "            sentences_df = sentences_df.append(new_row, ignore_index=True)\n",
    "        ctr += 1\n",
    "    sentences_df = sentences_df[(sentences_df.review_text.str.len() >= 20) & (sentences_df.review_text.str.len() <= 350)]\n",
    "    return sentences_df\n",
    "\n",
    "def embedInputs(books_df, review_df, search_param, review_max_len, searchTitle=True):\n",
    "    '''\n",
    "    Converts input reviews into USE arrays. Returns vectorized reviews for the book that was\n",
    "    passed as bookTitle.\n",
    "\n",
    "    Args:\n",
    "        search_param = List of book titles or author names whose reviews we want to embed. For authors, see 'searchTitle' argument\n",
    "        books_df = DataFrame with book_id information\n",
    "        review_df = DataFrame with book_id and review text\n",
    "        searchTitle = If True, will search for book_id based on title of a book. If False, it will look for author names to find book_id.\n",
    "    '''\n",
    "    if searchTitle:\n",
    "        #Finds book_id from title of book\n",
    "        input_book_id = books_df[books_df.title.isin([search_param])].book_id.tolist()\n",
    "        author_name = 'VariousAuthors/'\n",
    "    else:\n",
    "        # Finds book_id from author name\n",
    "        input_book_id = books_df[books_df.name.isin([search_param])].book_id.tolist()\n",
    "        author_name = books_df[books_df.name.isin([search_param])].name.iloc[0]\n",
    "\n",
    "    # Finds reviews for specified book\n",
    "    # input_sentences = review_df[review_df.book_id.isin(input_book_id)].review_text\n",
    "    input_sentences = cleanAndTokenize(review_df[review_df.book_id.isin(input_book_id)], tokenizedData, searchTitle=searchTitle, author=author_name).review_text\n",
    "\n",
    "    # Filters review length\n",
    "    input_sentences = input_sentences[input_sentences.str.len() <= review_max_len]\n",
    "\n",
    "    # Converts reviews into 512-dimensional arrays\n",
    "    input_vectors = embed(input_sentences)\n",
    "\n",
    "    # Returns reviews and vectorized reviews for a particular book\n",
    "    return input_sentences, input_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[t-SNE] Computing 141 nearest neighbors...\n",
      "[t-SNE] Indexed 142 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 142 samples in 0.007s...\n",
      "[t-SNE] Computed conditional probabilities for sample 142 / 142\n",
      "[t-SNE] Mean sigma: 0.474998\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 131.341644\n",
      "[t-SNE] KL divergence after 5000 iterations: 1.313684\n",
      "Opinion clusters about *Children of Time*\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "shapes (3,) and (512,142) not aligned: 3 (dim 0) != 512 (dim 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-df3d36786965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetClusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshowClusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Children of Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchTitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-e8673159dd3d>\u001b[0m in \u001b[0;36mshowClusters\u001b[0;34m(input_sentences, input_vectors, authorTitle, n_clusters, n_results, model, searchTitle)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mcentre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0minner_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_product\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mclusteredInputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (512,142) not aligned: 3 (dim 0) != 512 (dim 0)"
     ]
    }
   ],
   "source": [
    "model = getClusters(input_vectors, 8)\n",
    "indices, inner_product = showClusters(input_sentences, input_vectors, 'Children of Time', 8, 8, model, searchTitle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "inner_product.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-30 22:57:16.485 INFO    absl: resolver HttpCompressedFileResolver does not support the provided handle.\n",
      "2020-11-30 22:57:16.486 INFO    absl: resolver GcsCompressedFileResolver does not support the provided handle.\n",
      "2020-11-30 22:57:16.487 INFO    absl: resolver HttpUncompressedFileResolver does not support the provided handle.\n"
     ]
    }
   ],
   "source": [
    "# Paths to books and reviews DataFrames\n",
    "datapath = '/media/einhard/Seagate Expansion Drive/3380_data/data/'\n",
    "\n",
    "# Stores tokenized reviews so they only need to be processed the first time that particular book is called\n",
    "tokenizedData = '/media/einhard/Seagate Expansion Drive/3380_data/data/'\n",
    "books_file = 'Filtered books/clean_filtered_books.csv'\n",
    "reviews_file = 'Filtered books/clean_filtered_reviews.csv'\n",
    "reviewsAll_file = 'Filtered books/reviews_for_cluster.csv'\n",
    "\n",
    "# Loading DataFrames\n",
    "books, reviews, reviewsAll = dataLoader(datapath, books_file, reviews_file, reviewsAll_file)\n",
    "\n",
    "# Loadding pre-trained embeddings and embedder for input sentences\n",
    "embed, reviews_array, descriptions_array = loadEmbeddings()\n",
    "\n",
    "# Setting base URL for Goodreads\n",
    "goodreadsURL = 'https://www.goodreads.com/book/show/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hope for the Flowers']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "books.sample(1).title.tolist()"
   ]
  },
  {
   "source": [
    "# Testing full pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from langdetect import detect\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'Children of Time'\n",
    "book_title = books[books.title.str.contains(input_text, case=False)].title.tolist()\n",
    "input_book_id = books[books.title.isin(book_title)].book_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[25499718]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "input_book_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsAll[reviewsAll.book_id.isin([input_book_id])]\n",
    "\n",
    "\n",
    "\n",
    "input_sentences = cleanAndTokenize(reviewsAll[reviewsAll.book_id.isin(input_book_id)], tokenizedData, searchTitle=True, author='Adrian Tchaikovsky').review_text\n",
    "\n",
    "# Converts reviews into 512-dimensional arrays\n",
    "input_vectors = embed(input_sentences)\n",
    "# Returns reviews and vectorized reviews for a particular book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                                 Really liked this one.\n",
       "1                Fantastic and original science fiction.\n",
       "2                            Setting the stage for more.\n",
       "3                            I really enjoyed this book.\n",
       "4      I read a lot of sci-fi and this was up there w...\n",
       "                             ...                        \n",
       "137    Just about the right balance between science a...\n",
       "138    Yesterday I had to break a tiny web that had a...\n",
       "139                    \"I'm sorry, Portia,\" I whispered.\n",
       "140    I thought the book dragged a bit in the middle...\n",
       "141         It just pushed all the right buttons for me.\n",
       "Name: review_text, Length: 142, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "input_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(142, 512), dtype=float32, numpy=\n",
       "array([[-0.03793607,  0.04623649, -0.00505508, ..., -0.04547682,\n",
       "         0.00827182, -0.00052418],\n",
       "       [-0.04669672,  0.03274052, -0.07346593, ..., -0.04710089,\n",
       "         0.09411614,  0.04661921],\n",
       "       [ 0.02117739, -0.02780496,  0.01408575, ..., -0.00671999,\n",
       "         0.07057461,  0.01192718],\n",
       "       ...,\n",
       "       [-0.02475809,  0.023139  ,  0.006002  , ...,  0.00268129,\n",
       "         0.08098436,  0.0375176 ],\n",
       "       [-0.04929401, -0.05269561,  0.05049564, ..., -0.03259405,\n",
       "         0.11280895,  0.03305841],\n",
       "       [-0.0424552 ,  0.00778071,  0.0133794 , ..., -0.00455391,\n",
       "        -0.00720548, -0.03733094]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "input_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Opinion clusters about *Children of Time*\nEUCLEDIAN DISTANCES \n [[0.97986721 0.6898791  1.17845609 ... 1.13109286 0.93589187 1.15141068]\n [1.06466306 1.05215896 1.15893397 ... 1.10636515 1.01618467 1.10651268]\n [0.81628725 0.96221752 1.19189276 ... 1.15539174 0.95476636 1.13788915]\n ...\n [0.95494688 1.01326106 1.16443593 ... 1.15406938 0.87793586 1.14085719]\n [1.05721386 0.80401717 1.07956127 ... 1.11902202 1.06704623 1.12727826]\n [0.91293835 0.97519208 1.03292782 ... 1.05985123 0.82227564 0.91017246]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-61d40c108ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshowClusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Children of Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchTitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-b0dec49f63d7>\u001b[0m in \u001b[0;36mshowClusters\u001b[0;34m(input_sentences, input_vectors, authorTitle, n_clusters, n_results, model, searchTitle)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0minner_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'EUCLEDIAN DISTANCES \\n {inner_product}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_product\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mclusteredInputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4e955dab78a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m input_sentences, input_vectors = embedInputs(books,\n\u001b[1;32m      2\u001b[0m                                             \u001b[0mreviewsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                             \u001b[0msearch_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                                             \u001b[0mreview_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreview_max_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             searchTitle=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "input_sentences, input_vectors = embedInputs(books,\n",
    "                                            reviewsAll,\n",
    "                                            search_param=info.title.tolist()[0],\n",
    "                                            review_max_len=review_max_len,\n",
    "                                            searchTitle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Tokenizing reviews into sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenizea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Adrian Tchaikovsky'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "search_param = books[books.title == 'Children of Time'].name.tolist()[0]\n",
    "books[books.name.isin([search_param])].name.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 title  \\\n",
       "309  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "329  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "342  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "\n",
       "                                           description   book_id  \\\n",
       "309  Harry Potter's life is miserable. His parents ...         3   \n",
       "329  An alternative cover for this ASIN can be foun...  28132722   \n",
       "342  Harry Potter thinks he is an ordinary boy - un...  13562891   \n",
       "\n",
       "     weighted_score          name  \n",
       "309        4.449994  J.K. Rowling  \n",
       "329        4.448603  J.K. Rowling  \n",
       "342        4.446682  J.K. Rowling  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>description</th>\n      <th>book_id</th>\n      <th>weighted_score</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>309</th>\n      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n      <td>Harry Potter's life is miserable. His parents ...</td>\n      <td>3</td>\n      <td>4.449994</td>\n      <td>J.K. Rowling</td>\n    </tr>\n    <tr>\n      <th>329</th>\n      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n      <td>An alternative cover for this ASIN can be foun...</td>\n      <td>28132722</td>\n      <td>4.448603</td>\n      <td>J.K. Rowling</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n      <td>Harry Potter thinks he is an ordinary boy - un...</td>\n      <td>13562891</td>\n      <td>4.446682</td>\n      <td>J.K. Rowling</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "books[books.title.str.contains(\"Harry Potter and the Sorcerer's Stone\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         book_id                                        review_text\n",
       "169            3  Tuve el gusto de leerlo antes de que saliera l...\n",
       "214            3  I remember trying 3 times to read this but I a...\n",
       "1246           3            this was not the best harry potter book\n",
       "1793           3  I love the Harry Potter series, even though th...\n",
       "4229           3  One of the best and most magical children's bo...\n",
       "...          ...                                                ...\n",
       "1391054        3           decent wizarding book sequels are better\n",
       "1391095        3  Just started this, but a little hesitant to ge...\n",
       "1391392        3     Good...not as exciting as the others though...\n",
       "1391659        3  The start of Harry's adventure in Hogwarts. My...\n",
       "1392171        3  tahun pertama harry di hogwarts.. \\n draco seb...\n",
       "\n",
       "[3562 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>review_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>169</th>\n      <td>3</td>\n      <td>Tuve el gusto de leerlo antes de que saliera l...</td>\n    </tr>\n    <tr>\n      <th>214</th>\n      <td>3</td>\n      <td>I remember trying 3 times to read this but I a...</td>\n    </tr>\n    <tr>\n      <th>1246</th>\n      <td>3</td>\n      <td>this was not the best harry potter book</td>\n    </tr>\n    <tr>\n      <th>1793</th>\n      <td>3</td>\n      <td>I love the Harry Potter series, even though th...</td>\n    </tr>\n    <tr>\n      <th>4229</th>\n      <td>3</td>\n      <td>One of the best and most magical children's bo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1391054</th>\n      <td>3</td>\n      <td>decent wizarding book sequels are better</td>\n    </tr>\n    <tr>\n      <th>1391095</th>\n      <td>3</td>\n      <td>Just started this, but a little hesitant to ge...</td>\n    </tr>\n    <tr>\n      <th>1391392</th>\n      <td>3</td>\n      <td>Good...not as exciting as the others though...</td>\n    </tr>\n    <tr>\n      <th>1391659</th>\n      <td>3</td>\n      <td>The start of Harry's adventure in Hogwarts. My...</td>\n    </tr>\n    <tr>\n      <th>1392171</th>\n      <td>3</td>\n      <td>tahun pertama harry di hogwarts.. \\n draco seb...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3562 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "reviewsAll[reviewsAll.book_id.isin([3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Keeping only English"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(df):\n",
    "\n",
    "    # Read in CSV as dataframe\n",
    "    length_orig = len(df)\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    num_dups = length_orig - len(df)\n",
    "\n",
    "    print(f'Read in {length_orig} reviews, dropping {num_dups} duplicates\\n')\n",
    "\n",
    "    # Define spoiler marker & remove from all reviews\n",
    "    spoiler_str_ucsd = '\\*\\* spoiler alert \\*\\* \\n'\n",
    "    df['review_text'] = df['review_text'].str.replace(spoiler_str_ucsd, '')\n",
    "\n",
    "    # Replace all new line characters\n",
    "    df['review_text'] = df['review_text'].str.replace('\\n', ' ')\n",
    "\n",
    "    # Append space to all sentence end characters\n",
    "    df['review_text'] = df['review_text'].str.replace('.', '. ').replace('!', '! ').replace('?', '? ')\n",
    "\n",
    "    # Initialize dataframe to store English-language reviews\n",
    "    reviews_df = pd.DataFrame()\n",
    "    # Initialize counter for dropped reviews\n",
    "    drop_ctr = 0\n",
    "\n",
    "    # Loop through each row in dataframe\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Save review to variable\n",
    "        review = df.iloc[i]['review_text']\n",
    "\n",
    "        # Check if review is English\n",
    "        try:\n",
    "            if detect(review) == 'en':\n",
    "                # If so, add row to English-language dataframe\n",
    "                reviews_df = reviews_df.append(df.iloc[i, :])\n",
    "            else:\n",
    "                # If not, add 1 to dropped review counter\n",
    "                drop_ctr += 1\n",
    "        # If check fails, add 1 to dropped review counter\n",
    "        except:\n",
    "            drop_ctr += 1\n",
    "\n",
    "    reviews_df.book_id = reviews_df.book_id.astype(int)\n",
    "\n",
    "    print(f'Dropped {drop_ctr} non-English reviews. '\n",
    "          f'{len(reviews_df)} reviews remain.\\n')\n",
    "\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read in 3562 reviews, dropping 0 duplicates\n",
      "\n",
      "Dropped 189 non-English reviews. 1480 reviews remain.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         book_id                                        review_text\n",
       "1246           3            this was not the best harry potter book\n",
       "5361           3  The beginning of an inspiring literary traditi...\n",
       "5573           3  So much love for JK Rowling and her harry pott...\n",
       "6964           3  An excellent book, great for kids, not sure I'...\n",
       "7329           3   Love it as well as all the Harry Potter series. \n",
       "...          ...                                                ...\n",
       "1389325        3                   I love the Harry Potter series. \n",
       "1390184        3  I enjoyed Reading this book again.  Will conti...\n",
       "1390717        3  a beautiful adventure   wanna be with harry po...\n",
       "1390777        3  Sums up a child's experiences going into the r...\n",
       "1391392        3  Good. . . not as exciting as the others though...\n",
       "\n",
       "[1480 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>review_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1246</th>\n      <td>3</td>\n      <td>this was not the best harry potter book</td>\n    </tr>\n    <tr>\n      <th>5361</th>\n      <td>3</td>\n      <td>The beginning of an inspiring literary traditi...</td>\n    </tr>\n    <tr>\n      <th>5573</th>\n      <td>3</td>\n      <td>So much love for JK Rowling and her harry pott...</td>\n    </tr>\n    <tr>\n      <th>6964</th>\n      <td>3</td>\n      <td>An excellent book, great for kids, not sure I'...</td>\n    </tr>\n    <tr>\n      <th>7329</th>\n      <td>3</td>\n      <td>Love it as well as all the Harry Potter series.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1389325</th>\n      <td>3</td>\n      <td>I love the Harry Potter series.</td>\n    </tr>\n    <tr>\n      <th>1390184</th>\n      <td>3</td>\n      <td>I enjoyed Reading this book again.  Will conti...</td>\n    </tr>\n    <tr>\n      <th>1390717</th>\n      <td>3</td>\n      <td>a beautiful adventure   wanna be with harry po...</td>\n    </tr>\n    <tr>\n      <th>1390777</th>\n      <td>3</td>\n      <td>Sums up a child's experiences going into the r...</td>\n    </tr>\n    <tr>\n      <th>1391392</th>\n      <td>3</td>\n      <td>Good. . . not as exciting as the others though...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1480 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "clean_reviews(reviewsAll[reviewsAll.book_id.isin([3])], 30, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentences(reviews_df):\n",
    "    '''\n",
    "    Copyright (c) 2020 Willie Costello\n",
    "    '''\n",
    "    # Initialize dataframe to store review sentences, and counter\n",
    "    sentences_df = pd.DataFrame()\n",
    "    ctr = 0\n",
    "\n",
    "    print(f'Starting tokenization')\n",
    "\n",
    "    # Loop through each review\n",
    "    for i in range(len(reviews_df)):\n",
    "\n",
    "        # Save row and review to variables\n",
    "        row = reviews_df.iloc[i]\n",
    "        review = row.loc['review_text']\n",
    "\n",
    "        # Tokenize review into sentences\n",
    "        sentences = sent_tokenize(review)\n",
    "\n",
    "        # Loop through each sentence in list of tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            # Add row for sentence to sentences dataframe\n",
    "            new_row = row.copy()\n",
    "            new_row.at['review_text'] = sentence\n",
    "            sentences_df = sentences_df.append(new_row, ignore_index=True)\n",
    "\n",
    "        ctr += 1\n",
    "        if (ctr % 500 == 0):\n",
    "            print(f'{ctr} reviews tokenized')\n",
    "\n",
    "    sentences_df = sentences_df[(sentences_df.review_text.str.len() > 20) & (sentences_df.review_text.str.len() < 160)]\n",
    "    print(f'Tokenization complete: {len(sentences_df)} sentences tokenized\\n')\n",
    "\n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedData = '/media/einhard/Seagate Expansion Drive/3380_data/data/app_data/'\n",
    "def cleanAndTokenize(df, filepath):\n",
    "    if Path(filepath + df.book_id.iloc[1].astype(str) + '.csv').is_file():\n",
    "        sentences_df = pd.read_csv(filepath + df.book_id.iloc[1].astype(str) + '.csv').drop('Unnamed: 0', axis=1)\n",
    "    else:\n",
    "        reviews_df = clean_reviews(df)\n",
    "        sentences_df =  make_sentences(reviews_df)\n",
    "        sentences_df.to_csv(filepath + df.book_id.iloc[1].astype(str) + '.csv')\n",
    "    sentences_df.book_id = sentences_df.book_id.astype(int)\n",
    "    return sentences_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      book_id                                        review_text\n",
       "0           3  I remember trying 3 times to read this but I a...\n",
       "1           3  So I skipped to book 2 and then read the whole...\n",
       "2           3  It's hard to review this book without taking i...\n",
       "3           3            this was not the best harry potter book\n",
       "4           3  I love the Harry Potter series, even though th...\n",
       "...       ...                                                ...\n",
       "6915        3  I love the movies and am looking forward to ge...\n",
       "6916        3            Twi-hard/Potterhead in the making baby!\n",
       "6917        3              not as exciting as the others though.\n",
       "6918        3        The start of Harry's adventure in Hogwarts.\n",
       "6919        3               My most favorite book in the series!\n",
       "\n",
       "[6920 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>review_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>I remember trying 3 times to read this but I a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>So I skipped to book 2 and then read the whole...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>It's hard to review this book without taking i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>this was not the best harry potter book</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>I love the Harry Potter series, even though th...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6915</th>\n      <td>3</td>\n      <td>I love the movies and am looking forward to ge...</td>\n    </tr>\n    <tr>\n      <th>6916</th>\n      <td>3</td>\n      <td>Twi-hard/Potterhead in the making baby!</td>\n    </tr>\n    <tr>\n      <th>6917</th>\n      <td>3</td>\n      <td>not as exciting as the others though.</td>\n    </tr>\n    <tr>\n      <th>6918</th>\n      <td>3</td>\n      <td>The start of Harry's adventure in Hogwarts.</td>\n    </tr>\n    <tr>\n      <th>6919</th>\n      <td>3</td>\n      <td>My most favorite book in the series!</td>\n    </tr>\n  </tbody>\n</table>\n<p>6920 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "cleanAndTokenize(reviewsAll[reviewsAll.book_id.isin([3])], filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read in 3562 reviews, dropping 0 duplicates\n",
      "\n",
      "Dropped 187 non-English reviews. 3375 reviews remain.\n",
      "\n",
      "Starting tokenization\n",
      "500 reviews tokenized\n",
      "1000 reviews tokenized\n",
      "1500 reviews tokenized\n",
      "2000 reviews tokenized\n",
      "2500 reviews tokenized\n",
      "3000 reviews tokenized\n",
      "Tokenization complete: 7802 sentences tokenized\n",
      "\n",
      "Execution time in seconds: 39.682156562805176\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "reviewsCleaned = clean_reviews(reviewsAll[reviewsAll.book_id.isin([3])])\n",
    "reviewsTokenized = make_sentences(reviewsCleaned, 20, 160)\n",
    "\n",
    "\n",
    "reviewsEmbedded = embed(reviewsTokenized.review_text)\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/media/einhard/Seagate Expansion Drive/3380_data/data/app_data/3.csv\n"
     ]
    }
   ],
   "source": [
    "print(filepath + reviewsAll[reviewsAll.book_id.isin([3])].book_id.iloc[1].astype(str) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsTokenized.book_id = reviewsTokenized.book_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "reviewsAll[reviewsAll.book_id.isin([3])].book_id.iloc[1].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\"]"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Execution time in seconds: 0.8115620613098145\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "embed(reviewsTokenized.review_text)\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "source": [
    "# Testing spaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import Sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      book_id                                        review_text\n",
       "0         3.0  Tuve el gusto de leerlo antes de que saliera l...\n",
       "1         3.0  I remember trying 3 times to read this but I a...\n",
       "2         3.0  So I skipped to book 2 and then read the whole...\n",
       "3         3.0  It's hard to review this book without taking i...\n",
       "4         3.0            this was not the best harry potter book\n",
       "...       ...                                                ...\n",
       "8625      3.0     Good...not as exciting as the others though...\n",
       "8626      3.0        The start of Harry's adventure in Hogwarts.\n",
       "8627      3.0               My most favorite book in the series!\n",
       "8628      3.0                                WINGARDIUM LEVIOSA!\n",
       "8629      3.0  tahun pertama harry di hogwarts.. \\n draco seb...\n",
       "\n",
       "[8630 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>review_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.0</td>\n      <td>Tuve el gusto de leerlo antes de que saliera l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.0</td>\n      <td>I remember trying 3 times to read this but I a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>So I skipped to book 2 and then read the whole...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>It's hard to review this book without taking i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.0</td>\n      <td>this was not the best harry potter book</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8625</th>\n      <td>3.0</td>\n      <td>Good...not as exciting as the others though...</td>\n    </tr>\n    <tr>\n      <th>8626</th>\n      <td>3.0</td>\n      <td>The start of Harry's adventure in Hogwarts.</td>\n    </tr>\n    <tr>\n      <th>8627</th>\n      <td>3.0</td>\n      <td>My most favorite book in the series!</td>\n    </tr>\n    <tr>\n      <th>8628</th>\n      <td>3.0</td>\n      <td>WINGARDIUM LEVIOSA!</td>\n    </tr>\n    <tr>\n      <th>8629</th>\n      <td>3.0</td>\n      <td>tahun pertama harry di hogwarts.. \\n draco seb...</td>\n    </tr>\n  </tbody>\n</table>\n<p>8630 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "reviewsTokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsTokenized.to_csv('/media/einhard/Seagate Expansion Drive/3380_data/data/tokenized sentences/reviewsTokenized.csv')"
   ]
  },
  {
   "source": [
    "# Experimental"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To process embeddings\n",
    "import tensorflow_hub as hub\n",
    "from langdetect import detect\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# To create recommendations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# To create sentence clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# To load saved embeddings\n",
    "import joblib\n",
    "\n",
    "# To match strings\n",
    "import re\n",
    "\n",
    "# To create webapp\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental\n",
    "def findSemanticallySimilarReviews(query,reviews, books, sentence_array,  n_books):\n",
    "    # Create vector from query and compare with global embedding\n",
    "    sentence = [query]\n",
    "    sentence_vector = np.array(embed(sentence))\n",
    "    inner_product = np.inner(sentence_vector, sentence_array)[0]\n",
    "\n",
    "    # Find sentences with highest inner products\n",
    "    top_n_sentences = pd.Series(inner_product).nlargest(n_books+1)\n",
    "    top_n_indices = top_n_sentences.index.tolist()\n",
    "    review_index = reviews.iloc[top_n_indices].index\n",
    "\n",
    "    books_beta = books[books.book_id.isin(reviews.iloc[review_index].book_id)].sort_values('book_id')\n",
    "    reviews_beta = reviews.iloc[review_index].sort_values('book_id')\n",
    "\n",
    "    return books_beta, reviews_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_result = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_, reviews_ = findSemanticallySimilarReviews(query='Charlemagne',\n",
    "                                               reviews=reviewsAll,\n",
    "                                               books=books,\n",
    "                                               n_books=10,\n",
    "                                               sentence_array=reviews_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         book_id                                        review_text\n",
       "211406     28187  A reread. (I'm going to be rereading this seri...\n",
       "146200     34498     listening as always, stephen briggs is amazing\n",
       "324717     77203  This is a very emotional read. It was difficul...\n",
       "420175     99561  Great book! Very well written, so many good li...\n",
       "99869     110392  I liked Dunford I even liked Henry but the rus...\n",
       "186002   7260188  I liked the first two books of the trilogy bet...\n",
       "124971   7992995  Enjoyed the book despite the cover. Definitely...\n",
       "40079   12218678  I did not care for this novel. I lost all resp...\n",
       "167426  12408238  I needed a soothing book after reading Lords o...\n",
       "360406  15717876  Tried a sample on amazon. It was too corny and...\n",
       "420533  18007564  Andy Weir, you magnificent bastard! Don't reme..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>review_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>211406</th>\n      <td>28187</td>\n      <td>A reread. (I'm going to be rereading this seri...</td>\n    </tr>\n    <tr>\n      <th>146200</th>\n      <td>34498</td>\n      <td>listening as always, stephen briggs is amazing</td>\n    </tr>\n    <tr>\n      <th>324717</th>\n      <td>77203</td>\n      <td>This is a very emotional read. It was difficul...</td>\n    </tr>\n    <tr>\n      <th>420175</th>\n      <td>99561</td>\n      <td>Great book! Very well written, so many good li...</td>\n    </tr>\n    <tr>\n      <th>99869</th>\n      <td>110392</td>\n      <td>I liked Dunford I even liked Henry but the rus...</td>\n    </tr>\n    <tr>\n      <th>186002</th>\n      <td>7260188</td>\n      <td>I liked the first two books of the trilogy bet...</td>\n    </tr>\n    <tr>\n      <th>124971</th>\n      <td>7992995</td>\n      <td>Enjoyed the book despite the cover. Definitely...</td>\n    </tr>\n    <tr>\n      <th>40079</th>\n      <td>12218678</td>\n      <td>I did not care for this novel. I lost all resp...</td>\n    </tr>\n    <tr>\n      <th>167426</th>\n      <td>12408238</td>\n      <td>I needed a soothing book after reading Lords o...</td>\n    </tr>\n    <tr>\n      <th>360406</th>\n      <td>15717876</td>\n      <td>Tried a sample on amazon. It was too corny and...</td>\n    </tr>\n    <tr>\n      <th>420533</th>\n      <td>18007564</td>\n      <td>Andy Weir, you magnificent bastard! Don't reme...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "reviews_"
   ]
  },
  {
   "source": [
    "# Topic Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come', ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)\n",
    "\n",
    "def process_words(texts,bigram_mod, trigram_mod, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "def format_topics_sentences(corpus, texts, ldamodel=None,):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "def findTopic(reviewCluster):\n",
    "\n",
    "    # Convert to list\n",
    "    #data = reviewCluster.values.tolist()\n",
    "    data_words = list(sent_to_words(reviewCluster))\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    data_ready = process_words(data_words, bigram_mod=bigram_mod,trigram_mod=trigram_mod)\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "    # Create Corpus: Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=4, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=10,\n",
    "                                               passes=10,\n",
    "                                               alpha='symmetric',\n",
    "                                               iterations=100,\n",
    "                                               per_word_topics=True)\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "\n",
    "    sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "    sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "    for i, grp in sent_topics_outdf_grpd:\n",
    "        sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                                 grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                                axis=0)\n",
    "\n",
    "    # Reset Index    \n",
    "    sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Format\n",
    "    sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "    # Show\n",
    "    return sent_topics_sorteddf_mallet['Representative Text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['enjoy',\n",
       " 'book',\n",
       " 'cover',\n",
       " 'definitely',\n",
       " 'triumph',\n",
       " 'evil',\n",
       " 'theme',\n",
       " 'like',\n",
       " 'little',\n",
       " 'strong',\n",
       " 'female',\n",
       " 'heroine']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "findTopic(reviews_['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}