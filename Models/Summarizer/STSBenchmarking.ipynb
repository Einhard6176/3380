{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To process embeddings\n",
    "import tensorflow_hub as hub\n",
    "from langdetect import detect\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# To create recommendations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# To create sentence clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# To load saved embeddings\n",
    "import joblib\n",
    "\n",
    "# To match strings\n",
    "import re\n",
    "\n",
    "# To create webapp\n",
    "import psutil\n",
    "import streamlit as st\n",
    "from streamlit import caching\n",
    "\n",
    "\n",
    "# To sort final recommendation list\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def dataLoader(datapath, books_file, reviewsAll_file, tokenized_descriptions_file):\n",
    "    '''\n",
    "    Loads DataFrames with books and review information\n",
    "\n",
    "    Args:\n",
    "        datapath = Path/to/directory/with/data\n",
    "        books_file = name of CSV with book metadata\n",
    "        reviewsALL_file = name of CSV with filtered reviews --> Generally, this is the file that will be used in the app.\n",
    "    '''\n",
    "    books = pd.read_csv(datapath + books_file).drop('Unnamed: 0', axis=1).fillna('')\n",
    "    reviewsAll = pd.read_csv(datapath + reviewsAll_file).drop('Unnamed: 0', axis=1)\n",
    "    description_sentences = pd.read_csv(datapath + tokenized_descriptions_file)\n",
    "    return books, reviewsAll, description_sentences\n",
    "\n",
    "\n",
    "def loadEmbeddings(datapath):\n",
    "    '''\n",
    "    Loads pre-trained sentence and review arrays\n",
    "\n",
    "    Args:\n",
    "        datapath = Path/to/directory/with/data\n",
    "    '''\n",
    "    # Path to USE\n",
    "    embed = hub.load(datapath + 'tensorflow_hub/universal-sentence-encoder_4')\n",
    "\n",
    "    # Load pre-trained sentence arrays\n",
    "    ## Reviews array is a set of embeddings trained on review lengths of < 90 characters\n",
    "    #reviews_array = joblib.load(datapath + 'Models/reviewEmbeddings.pkl')\n",
    "    # Descriptions array is a set of embeddings trained on all book descriptions\n",
    "    descriptions_array = joblib.load('/media/einhard/Seagate Expansion Drive/3380_data/data/Models/descriptionsTokeniziedEmbedding.pkl')\n",
    "\n",
    "    return embed, descriptions_array # reviews_array,\n",
    "\n",
    "#################### Basic Clustering Functionality ####################\n",
    "\n",
    "\n",
    "def embedInputs(books_df, review_df, search_param, review_max_len, searchTitle=True):\n",
    "    '''\n",
    "    Converts input reviews into USE arrays. Returns vectorized reviews for the book that was\n",
    "    passed as bookTitle.\n",
    "\n",
    "    Args:\n",
    "        search_param = List of book titles or author names whose reviews we want to embed. For authors, see 'searchTitle' argument\n",
    "        books_df = DataFrame with book_id information\n",
    "        review_df = DataFrame with book_id and review text\n",
    "        searchTitle = If True, will search for book_id based on title of a book. If False, it will look for author names to find book_id.\n",
    "    '''\n",
    "    if searchTitle:\n",
    "        #Finds book_id from title of book\n",
    "        input_book_id = books_df[books_df.title.isin([search_param])].book_id.tolist()\n",
    "        author_name = 'VariousAuthors/'\n",
    "    else:\n",
    "        # Finds book_id from author name\n",
    "        input_book_id = books_df[books_df.name.isin([search_param])].book_id.tolist()\n",
    "        author_name = books_df[books_df.name.isin([search_param])].name.iloc[0]\n",
    "\n",
    "    # Finds reviews for specified book\n",
    "    # input_sentences = review_df[review_df.book_id.isin(input_book_id)].review_text\n",
    "    input_sentences = cleanAndTokenize(review_df[review_df.book_id.isin(input_book_id)], tokenizedData, searchTitle=searchTitle, author=author_name).review_text\n",
    "\n",
    "    # Filters review length\n",
    "    input_sentences = input_sentences[input_sentences.str.len() <= review_max_len]\n",
    "\n",
    "    # Converts reviews into 512-dimensional arrays\n",
    "    input_vectors = embed(input_sentences)\n",
    "\n",
    "    # Returns reviews and vectorized reviews for a particular book\n",
    "    return input_sentences, input_vectors\n",
    "\n",
    "# Pass \"sentence_array\" as input_vectors to compare with user input???????\n",
    "\n",
    "def getClusters(input_vectors, n_clusters):\n",
    "    '''\n",
    "    Creates KMeans instance and fits model.\n",
    "\n",
    "    The for nested for loop is used to display the returned sentences on Streamlit.\n",
    "\n",
    "    Args:\n",
    "        input_sentences =  Sentences to compare\n",
    "        n_clusters = How many clusters to generate\n",
    "    '''\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, algorithm='full')\n",
    "    return kmeans.fit(input_vectors)\n",
    "\n",
    "\n",
    "def showClusters(input_sentences, input_vectors, authorTitle, n_clusters, n_results, model, searchTitle=True):\n",
    "    '''\n",
    "    This function will find theme clusters in the reviews of a particular book or set of sentences.\n",
    "    Uses cluster centers to find semantically similar sentences to the input vectors.\n",
    "\n",
    "    The nested for loop is used to display the returned sentences on Streamlit.\n",
    "\n",
    "    Args:\n",
    "        input_sentences =  Sentences to compare\n",
    "        input_vectors = USE Array generated by embedding input sentences\n",
    "        authorTitle = Title of book in question, or name of author --> Used to display header only\n",
    "        n_clusters = How many clusters to generate\n",
    "        n_results = How many sentences to display per n_cluster\n",
    "        model = The model used to create the clusters.\n",
    "    '''\n",
    "    if searchTitle:\n",
    "        # Displays which book's reviews are being clustered\n",
    "        st.header(f'Opinion clusters about *{authorTitle}*')\n",
    "    else:\n",
    "        st.header(f'Opinion clusters about {authorTitle}\\'s books')\n",
    "\n",
    "    # Iterates through centroids to and computes inner products to find nlargest\n",
    "    for i in range(n_clusters):\n",
    "        centre = model.cluster_centers_[i]\n",
    "        inner_product = np.inner(centre, input_vectors)\n",
    "        indices = pd.Series(inner_product).nlargest(n_results).index\n",
    "        clusteredInputs = list(input_sentences.iloc[indices])\n",
    "\n",
    "        # Writes reviews that are closest to centroid\n",
    "        st.markdown('---')\n",
    "        collapseCluster = st.beta_expander(f'Opinion cluster #{i+1}', expanded=True)\n",
    "        with collapseCluster:\n",
    "            for sentence in clusteredInputs:\n",
    "                st.write(sentence)\n",
    "\n",
    "\n",
    "#################### Tokenizing and saving data for embedding ####################\n",
    "\n",
    "\n",
    "def clean_reviews(df):\n",
    "    '''\n",
    "    Copyright (c) 2020 Willie Costello\n",
    "    '''\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    #Fill nans\n",
    "    df.fillna('la maquina solo puede en ingles', inplace=True)\n",
    "\n",
    "    # Define spoiler marker & remove from all reviews\n",
    "    spoiler_str_ucsd = '\\*\\* spoiler alert \\*\\* \\n'\n",
    "    df['review_text'] = df['review_text'].str.replace(spoiler_str_ucsd, '')\n",
    "\n",
    "    # Replace all new line characters\n",
    "    df['review_text'] = df['review_text'].str.replace('\\n', ' ')\n",
    "\n",
    "    # Append space to all sentence end characters\n",
    "    df['review_text'] = df['review_text'].str.replace('.', '. ').replace('!', '! ').replace('?', '? ')\n",
    "\n",
    "    # Initialize dataframe to store English-language reviews\n",
    "    reviews_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each row in dataframe\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Save review to variable\n",
    "        review = df.iloc[i]['review_text']\n",
    "        try:\n",
    "            # Check if review is English\n",
    "            if detect(review) == 'en':\n",
    "                # If so, add row to English-language dataframe\n",
    "                reviews_df = reviews_df.append(df.iloc[i, :])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    reviews_df.book_id = reviews_df.book_id.astype(int)\n",
    "    return reviews_df\n",
    "\n",
    "\n",
    "def make_sentences(reviews_df):\n",
    "    '''\n",
    "    Copyright (c) 2020 Willie Costello\n",
    "    '''\n",
    "    # Initialize dataframe to store review sentences, and counter\n",
    "    sentences_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each review\n",
    "    for i in range(len(reviews_df)):\n",
    "\n",
    "        # Save row and review to variables\n",
    "        row = reviews_df.iloc[i]\n",
    "        review = row.loc['review_text']\n",
    "\n",
    "        # Tokenize review into sentences\n",
    "        sentences = sent_tokenize(review)\n",
    "\n",
    "        # Loop through each sentence in list of tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            # Add row for sentence to sentences dataframe\n",
    "            new_row = row.copy()\n",
    "            new_row.at['review_text'] = sentence\n",
    "            sentences_df = sentences_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    sentences_df = sentences_df[(sentences_df.review_text.str.len() >= 20) & (sentences_df.review_text.str.len() <= 350)]\n",
    "    return sentences_df\n",
    "\n",
    "\n",
    "def cleanAndTokenize(df, filepath, searchTitle, author):\n",
    "    '''\n",
    "    Helper function that first checks if a CSV file has already been generated for that book or author.\n",
    "    If not, runs another cleaning pass on the set of reviews and tokenizes text into sentences before\n",
    "    saving a CSV file to speed up recalls for the same book alter.\n",
    "\n",
    "    Returns DataFrame with tokenized reviews for a particular author or book.\n",
    "\n",
    "    Args:\n",
    "        df          = dataframe to tokenize. It will save and load based on the first book_id if searching titles, or based on the author passed as an argument.\n",
    "        filepath    = Path to output folder. If if a file exits, it will look in this path for the CSV files. Subdirectories must be made for ./book_id/ and for ./author/\n",
    "        searchTitle = Specify whether the reviews for an individual book should be tokenized or the all reviews for an author.\n",
    "        author      = If searchTitle=False, the \"author\" argument is used for saving and loading files. Note that it has no effect on the tokenization itself.\n",
    "    '''\n",
    "\n",
    "    if searchTitle:\n",
    "        if Path(filepath + 'app_data/book_id/' + df.book_id.iloc[1].astype(str) + '.csv').is_file():\n",
    "            sentences_df = pd.read_csv(filepath + 'app_data/book_id/' + df.book_id.iloc[1].astype(str) + '.csv').drop('Unnamed: 0', axis=1)\n",
    "        else:\n",
    "            reviews_df = clean_reviews(df.fillna('la maquina no puede en ingles'))\n",
    "            sentences_df =  make_sentences(reviews_df)\n",
    "            sentences_df.to_csv(filepath + 'app_data/book_id/' + df.book_id.iloc[1].astype(str) + '.csv')\n",
    "        sentences_df.book_id = sentences_df.book_id.astype(int)\n",
    "        return sentences_df\n",
    "\n",
    "    else:\n",
    "        if Path(filepath + 'app_data/author/' + author + '.csv').is_file():\n",
    "            sentences_df = pd.read_csv(filepath + 'app_data/author/' + author + '.csv').drop('Unnamed: 0', axis=1)\n",
    "        else:\n",
    "            reviews_df = clean_reviews(df.fillna('la maquina no puede en ingles'))\n",
    "            sentences_df =  make_sentences(reviews_df)\n",
    "            sentences_df.to_csv(filepath + 'app_data/author/' + author + '.csv')\n",
    "        sentences_df.book_id = sentences_df.book_id.astype(int)\n",
    "\n",
    "        return sentences_df\n",
    "\n",
    "\n",
    "\n",
    "#################### Searching based on description or review ####################\n",
    "\n",
    "def findSimilarity(input_text, df, searchDescription):\n",
    "    pass\n",
    "\n",
    "def searchBookTitles(input_text, reviews, books, n_clusters, n_cluster_reviews):\n",
    "    pass\n",
    "\n",
    "def semanticSearch(input_text, n_books):\n",
    "    query = embed([input_text])\n",
    "    result = np.inner(descriptions_array, query)\n",
    "    iloc_desc = pd.DataFrame(result).sort_values(0, ascending=False).reset_index().rename({'index':'position'}, axis=1)['position'].tolist()\n",
    "    # iloc_books = description_sentences.iloc[iloc_desc][:10].book_id.tolist()\n",
    "    book_title = description_sentences.iloc[iloc_desc][:n_books].title.tolist()\n",
    "    return book_title\n",
    "\n",
    "# Basic TF-IDF cosine similarity engine\n",
    "\n",
    "def createSimilarities(books_df):\n",
    "    '''\n",
    "    Creates a similarity matrix for book recommendations based on description.\n",
    "    Returns similarity matrix and mapping for book-finding\n",
    "\n",
    "    Args:\n",
    "        books_df = DataFrame with books and descriptions\n",
    "    '''\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 5), min_df=0, stop_words='english')\n",
    "    tfidf_matrix = tf.fit_transform(books['description'])\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    mapping = pd.Series(books.index,index = books['title'])\n",
    "    return cosine_similarities, mapping\n",
    "\n",
    "\n",
    "def bookRecommendation(book_title, mapping, cosine_similarities, n_books):\n",
    "    '''\n",
    "    Function to match input book with recommended books using cosine similarity matrix.\n",
    "\n",
    "    Args:\n",
    "        book_title          = Input book title. Recommendations will be made based on this.\n",
    "        mapping             = Mapping of cosine similarity matrix to book indices\n",
    "        cosine_similarities = Similarity matrix for content based recommendation\n",
    "        n_books             = How many books to recommend\n",
    "    '''\n",
    "    book_index = mapping[book_title]\n",
    "    similarity_score = list(enumerate(cosine_similarities[book_index]))\n",
    "    similarity_score = sorted(similarity_score, key=lambda x: x[1], reverse=True)\n",
    "    similarity_score = similarity_score[1:n_books+1]\n",
    "    book_indices = [i[0] for i in similarity_score]\n",
    "    return (books['title'].iloc[book_indices])\n",
    "\n",
    "\n",
    "##### Experimental functionality, WIP #####\n",
    "\n",
    "#\n",
    "#def findSemanticallySimilarReviews(query,reviews, books, sentence_array,  n_books):\n",
    "#    # Create vector from query and compare with global embedding\n",
    "#    sentence = [query]\n",
    "#    sentence_vector = np.array(embed(sentence))\n",
    "#    inner_product = np.inner(sentence_vector, sentence_array)[0]\n",
    "#\n",
    "#    # Find sentences with highest inner products\n",
    "#    top_n_sentences = pd.Series(inner_product).nlargest(n_books+1)\n",
    "#    top_n_indices = top_n_sentences.index.tolist()\n",
    "#    book_titles = books[books.book_id.isin(reviews.iloc[top_n_indices].book_id.tolist())].title.tolist()\n",
    "#\n",
    "#    return book_titles, reviews.iloc[top_n_indices].index\n",
    "\n",
    "\n",
    "\n",
    "#################### App UI and Interactions ####################\n",
    "\n",
    "\n",
    "def showInfo(iterator, n_clusters, n_results,n_books, review_max_len=350):\n",
    "    with results:\n",
    "        for idx, i in enumerate(iterator[:n_books]):\n",
    "            try:\n",
    "                info = books[books.title == i]\n",
    "                '**---**'\n",
    "                '**Book title:**', f'*{info.title.tolist()[0]}*'\n",
    "                '**Author:**', info.name.tolist()[0]\n",
    "                '**Weighted Score**', str(round(info.weighted_score.tolist()[0], 2)), '/ 5'\n",
    "\n",
    "                showDescription = st.beta_expander(label='Show description?')\n",
    "                showReviewClusters = st.button(label='Show opinion clusters for this book?', key=idx)\n",
    "                showAuthorClusters = st.button(label='Show opinion clusters for this author?', key=idx+100)\n",
    "            #    showSimilarBooks = st.button(label='Show similar books?', key=idx+555)\n",
    "            except IndexError:\n",
    "                break\n",
    "            with showDescription:\n",
    "                st.write(info.description.tolist()[0])\n",
    "\n",
    "            if showReviewClusters:\n",
    "                with clusters:\n",
    "                    try:\n",
    "                        input_sentences, input_vectors = embedInputs(books,\n",
    "                                                                    reviewsAll,\n",
    "                                                                    search_param=info.title.tolist()[0],\n",
    "                                                                    review_max_len=review_max_len,\n",
    "                                                                    searchTitle=True)\n",
    "                        model = getClusters(input_vectors=input_vectors,\n",
    "                                            n_clusters=n_clusters)\n",
    "                        showClusters(input_sentences=input_sentences,\n",
    "                                    input_vectors=input_vectors,\n",
    "                                    authorTitle = info.title.tolist()[0],\n",
    "                                    n_clusters=n_clusters,\n",
    "                                    n_results=n_results,\n",
    "                                    model=model,\n",
    "                                    searchTitle=True)\n",
    "                    except ValueError:\n",
    "                        print(f\"It looks like this book doesn't have enough reviews to generate {n_clusters} distinct clusters. Try decreasing how many clusters you look for!\")\n",
    "                        continue\n",
    "            if showAuthorClusters:\n",
    "                with clusters:\n",
    "                    try:\n",
    "                        input_sentences, input_vectors = embedInputs(books,\n",
    "                                                                    reviewsAll,\n",
    "                                                                    search_param=info.name.tolist()[0],\n",
    "                                                                    review_max_len=review_max_len,\n",
    "                                                                    searchTitle=False)\n",
    "                        model = getClusters(input_vectors=input_vectors,\n",
    "                                            n_clusters=n_clusters)\n",
    "                        showClusters(input_sentences=input_sentences,\n",
    "                                    input_vectors=input_vectors,\n",
    "                                    authorTitle = info.name.tolist()[0],\n",
    "                                    n_clusters=n_clusters,\n",
    "                                    n_results=n_results,\n",
    "                                    model=model,\n",
    "                                    searchTitle=False)\n",
    "                    except ValueError:\n",
    "                        print(f\"It looks like this author's books don't have enough reviews to generate {n_clusters} distinct clusters. Try decreasing how many clusters you look for!\")\n",
    "                        continue\n",
    "\n",
    "            ##### Experimental functionality, WIP #####\n",
    "\n",
    "            #if showSimilarBooks:\n",
    "            #    showInfo(iterator=info.title.tolist()[0],\n",
    "            #            n_clusters=n_clusters,\n",
    "            #            n_results=n_results,\n",
    "            #            n_books=n_books,\n",
    "            #            review_max_len=review_max_len)\n",
    "\n",
    "            if goodreadsLink:\n",
    "                good_reads_link = goodreadsURL + info.book_id.astype(str).tolist()[0]\n",
    "                print(f'*Goodreads Link: {good_reads_link}*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-12-02 16:48:35.474 INFO    absl: resolver HttpCompressedFileResolver does not support the provided handle.\n",
      "2020-12-02 16:48:35.475 INFO    absl: resolver GcsCompressedFileResolver does not support the provided handle.\n",
      "2020-12-02 16:48:35.476 INFO    absl: resolver HttpUncompressedFileResolver does not support the provided handle.\n"
     ]
    }
   ],
   "source": [
    "# Paths to books and reviews DataFrames\n",
    "datapath = '/media/einhard/Seagate Expansion Drive/3380_data/data/'\n",
    "\n",
    "# Stores tokenized reviews so they only need to be processed the first time that particular book is called\n",
    "tokenizedData = '/media/einhard/Seagate Expansion Drive/3380_data/data/'\n",
    "books_file = 'Filtered books/clean_filtered_books.csv'\n",
    "reviewsAll_file = 'Filtered books/reviews_for_cluster.csv'\n",
    "descriptions_tokenized_file = 'Filtered books/description_sentences.csv'\n",
    "\n",
    "# Loading DataFrames\n",
    "books, reviewsAll, description_sentences = dataLoader(datapath, books_file, reviewsAll_file, descriptions_tokenized_file)\n",
    "\n",
    "# Loadding pre-trained embeddings and embedder for input sentences\n",
    "embed, descriptions_array = loadEmbeddings(datapath) # , reviews_array, descriptions_array\n",
    "\n",
    "# Setting base URL for Goodreads\n",
    "goodreadsURL = 'https://www.goodreads.com/book/show/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, clusters = st.beta_columns(2)\n",
    "goodreadsLink = st.checkbox('Show Goodreads links.')\n",
    "#book_title = books[books.title.str.contains(input_text, case=False)].sort_values('weighted_score', ascending=False).title.tolist()[0]\n",
    "book_title = semanticSearch('Gone Girl', 5)\n",
    "# with results:\n",
    "#     # st.markdown(f'## Book recommendations based on *{book_title}*')\n",
    "#     st.markdown('## Book recommendations based on your input:')\n",
    "#cosine_similarities, mapping = createSimilarities(books)\n",
    "#book_recommends = bookRecommendation(book_title=book_title,\n",
    "#                                    mapping=mapping,\n",
    "#                                    cosine_similarities=cosine_similarities,\n",
    "#                                    n_books=n_books)\n",
    "showInfo(iterator=book_title, #book_recommends,\n",
    "         n_clusters=8,\n",
    "         n_results=8,\n",
    "         n_books=5,\n",
    "         review_max_len=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-38dca78886d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m input_sentences, input_vectors = embedInputs(books,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                             \u001b[0mreviewsAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                             \u001b[0msearch_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Gone Girl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                             \u001b[0mreview_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             searchTitle=False)\n",
      "\u001b[0;32m<ipython-input-7-bf51cb20b388>\u001b[0m in \u001b[0;36membedInputs\u001b[0;34m(books_df, review_df, search_param, review_max_len, searchTitle)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Finds book_id from author name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0minput_book_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooks_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbooks_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msearch_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mauthor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooks_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbooks_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msearch_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# Finds reviews for specified book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "input_sentences, input_vectors = embedInputs(books,\n",
    "                                            reviewsAll,\n",
    "                                            search_param=['Gone Girl'],\n",
    "                                            review_max_len=350,\n",
    "                                            searchTitle=False)\n",
    "model = getClusters(input_vectors=input_vectors,\n",
    "                    n_clusters=n_clusters)\n",
    "showClusters(input_sentences=input_sentences,\n",
    "            input_vectors=input_vectors,\n",
    "            authorTitle = ['Gillian Flynn'],\n",
    "            n_clusters=8,\n",
    "            n_results=8,\n",
    "            model=model,\n",
    "            searchTitle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}