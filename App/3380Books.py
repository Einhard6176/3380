import numpy as np
import pandas as pd

# To process embeddings
import tensorflow_hub as hub
from langdetect import detect
from pathlib import Path
from nltk.tokenize import sent_tokenize

# To create sentence clusters
from sklearn.cluster import KMeans

# To load saved embeddings
import joblib

# To match strings
import re

# To create webapp
import psutil
import streamlit as st
st.set_page_config(layout="wide")
from streamlit import caching


# To sort final recommendation list
from collections import Counter

#######################################################################################
                            # Functions
#######################################################################################

#################### Data Loading  Functions ####################

@st.cache
def dataLoader(datapath, books_file, reviews_file, reviewsAll_file):
    '''
    Loads DataFrames with books and review information
    '''
    books = pd.read_csv(datapath + books_file).drop('Unnamed: 0', axis=1).fillna('')
    reviews = pd.read_csv(datapath + reviews_file).drop('Unnamed: 0', axis=1)
    reviewsAll = pd.read_csv(datapath + reviewsAll_file).drop('Unnamed: 0', axis=1)
    return books, reviews, reviewsAll

@st.cache
def loadEmbeddings():
    '''
    Loads pre-trained sentence and review arrays
    '''
    # Path to USE
    embed = hub.load('/media/einhard/Seagate Expansion Drive/3380_data/data/tensorflow_hub/universal-sentence-encoder_4')

    # Load pre-trained sentence arrays
    ## Reviews array is a set of embeddings trained on review lengths of < 90 characters
    reviews_array = joblib.load('/media/einhard/Seagate Expansion Drive/3380_data/data/Models/reviewEmbeddings.pkl')
    ## Descriptions array is a set of embeddings trained on all book descriptions
    descriptions_array = joblib.load('/media/einhard/Seagate Expansion Drive/3380_data/data/Models/descriptionEmbeddings.pkl')

    return embed, reviews_array, descriptions_array

#################### Basic Clustering Functionality ####################

@st.cache(allow_output_mutation=True)
def embedInputs(books_df, review_df, search_param, review_max_len, searchTitle=True):
    '''
    Converts input reviews into USE arrays. Returns vectorized reviews for the book that was
    passed as bookTitle.

    Args:
        search_param = List of book titles or author names whose reviews we want to embed. For authors, see 'searchTitle' argument
        books_df = DataFrame with book_id information
        review_df = DataFrame with book_id and review text
        searchTitle = If True, will search for book_id based on title of a book. If False, it will look for author names to find book_id.
    '''
    if searchTitle:
        #Finds book_id from title of book
        input_book_id = books_df[books_df.title.isin([search_param])].book_id.tolist()
        author_name = 'VariousAuthors/'
    else:
        # Finds book_id from author name
        input_book_id = books_df[books_df.name.isin([search_param])].book_id.tolist()
        author_name = books_df[books_df.name.isin([search_param])].name.iloc[0]

    # Finds reviews for specified book
    # input_sentences = review_df[review_df.book_id.isin(input_book_id)].review_text
    input_sentences = cleanAndTokenize(review_df[review_df.book_id.isin(input_book_id)], tokenizedData, searchTitle=searchTitle, author=author_name).review_text

    # Filters review length
    input_sentences = input_sentences[input_sentences.str.len() <= review_max_len]

    # Converts reviews into 512-dimensional arrays
    input_vectors = embed(input_sentences)

    # Returns reviews and vectorized reviews for a particular book
    return input_sentences, input_vectors

# Pass "sentence_array" as input_vectors to compare with user input???????
@st.cache
def getClusters(input_vectors, n_clusters):
    '''
    Creates KMeans instance and fits model.

    The for nested for loop is used to display the returned sentences on Streamlit.

    Args:
        input_sentences =  Sentences to compare
        n_clusters = How many clusters to generate
    '''
    kmeans = KMeans(n_clusters=n_clusters, n_init=50, algorithm='full')
    return kmeans.fit(input_vectors)


def showClusters(input_sentences, input_vectors, authorTitle, n_clusters, n_results, model, searchTitle=True):
    '''
    This function will find theme clusters in the reviews of a particular book or set of sentences.
    Uses cluster centers to find semantically similar sentences to the input vectors.

    The nested for loop is used to display the returned sentences on Streamlit.

    Args:
        input_sentences =  Sentences to compare
        input_vectors = USE Array generated by embedding input sentences
        authorTitle = Title of book in question, or name of author --> Used to display header only
        n_clusters = How many clusters to generate
        n_results = How many sentences to display per n_cluster
        model = The model used to create the clusters.
    '''
    if searchTitle:
        # Displays which book's reviews are being clustered
        st.header(f'Opinion clusters about *{authorTitle}*')
    else:
        st.header(f'Opinion clusters about {authorTitle}\'s books')

    # Iterates through centroids to and computes inner products to find nlargest
    for i in range(n_clusters):
        centre = model.cluster_centers_[i]
        inner_product = np.inner(centre, input_vectors)
        indices = pd.Series(inner_product).nlargest(n_results).index
        clusteredInputs = list(input_sentences.iloc[indices])

        # Prints reviews that are closest to centroid
        st.markdown('---')
        st.write(f'**Cluster #{i+1}**')
        for sentence in clusteredInputs:
            st.write(sentence)

#################### Tokenizing and saving data for embedding ####################

@st.cache
def clean_reviews(df):

    # Read in CSV as dataframe
    length_orig = len(df)

    # Drop duplicates
    df.drop_duplicates(inplace=True)
    num_dups = length_orig - len(df)

    print(f'Read in {length_orig} reviews, dropping {num_dups} duplicates\n')

    # Define spoiler marker & remove from all reviews
    spoiler_str_ucsd = '\*\* spoiler alert \*\* \n'
    df['review_text'] = df['review_text'].str.replace(spoiler_str_ucsd, '')

    # Replace all new line characters
    df['review_text'] = df['review_text'].str.replace('\n', ' ')

    # Append space to all sentence end characters
    df['review_text'] = df['review_text'].str.replace('.', '. ').replace('!', '! ').replace('?', '? ')

    # Initialize dataframe to store English-language reviews
    reviews_df = pd.DataFrame()
    # Initialize counter for dropped reviews
    drop_ctr = 0

    # Loop through each row in dataframe
    for i in range(len(df)):

        # Save review to variable
        review = df.iloc[i]['review_text']

        # Check if review is English
        try:
            if detect(review) == 'en':
                # If so, add row to English-language dataframe
                reviews_df = reviews_df.append(df.iloc[i, :])
            else:
                # If not, add 1 to dropped review counter
                drop_ctr += 1
        # If check fails, add 1 to dropped review counter
        except:
            drop_ctr += 1

    reviews_df.book_id = reviews_df.book_id.astype(int)

    print(f'Dropped {drop_ctr} non-English reviews. '
          f'{len(reviews_df)} reviews remain.\n')

    return reviews_df

@st.cache(allow_output_mutation=True)
def make_sentences(reviews_df):
    '''
    Copyright (c) 2020 Willie Costello
    '''
    # Initialize dataframe to store review sentences, and counter
    sentences_df = pd.DataFrame()
    ctr = 0

    print(f'Starting tokenization')

    # Loop through each review
    for i in range(len(reviews_df)):

        # Save row and review to variables
        row = reviews_df.iloc[i]
        review = row.loc['review_text']

        # Tokenize review into sentences
        sentences = sent_tokenize(review)

        # Loop through each sentence in list of tokenized sentences
        for sentence in sentences:
            # Add row for sentence to sentences dataframe
            new_row = row.copy()
            new_row.at['review_text'] = sentence
            sentences_df = sentences_df.append(new_row, ignore_index=True)

        ctr += 1
        if (ctr % 500 == 0):
            print(f'{ctr} reviews tokenized')

    sentences_df = sentences_df[(sentences_df.review_text.str.len() > 20) & (sentences_df.review_text.str.len() < 350)]
    print(f'Tokenization complete: {len(sentences_df)} sentences tokenized\n')

    return sentences_df

@st.cache(allow_output_mutation=True)
def cleanAndTokenize(df, filepath, searchTitle, author):
    if searchTitle:
        if Path(filepath + 'book_id/' + df.book_id.iloc[1].astype(str) + '.csv').is_file():
            sentences_df = pd.read_csv(filepath + 'book_id/' + df.book_id.iloc[1].astype(str) + '.csv').drop('Unnamed: 0', axis=1)
        else:
            reviews_df = clean_reviews(df)
            sentences_df =  make_sentences(reviews_df)
            sentences_df.to_csv(filepath + 'book_id/' + df.book_id.iloc[1].astype(str) + '.csv')
        sentences_df.book_id = sentences_df.book_id.astype(int)
        return sentences_df

    else:
        if Path(filepath + 'author/' + author + '.csv').is_file():
            sentences_df = pd.read_csv(filepath + 'author/' + author + '.csv').drop('Unnamed: 0', axis=1)
        else:
            reviews_df = clean_reviews(df)
            sentences_df =  make_sentences(reviews_df)
            sentences_df.to_csv(filepath + 'author/' + author + '.csv')
        sentences_df.book_id = sentences_df.book_id.astype(int)

        return sentences_df



#################### Searching based on description or review ####################

def findSimilarity(input_text, df, searchDescription):
    pass

def searchBookTitles(input_text, reviews, books, n_clusters, n_cluster_reviews):
    pass


#################### App UI and Interactions ####################


def showInfo(iterator, n_clusters, n_results,n_books, review_max_len=0):
    with results:
        for idx, i in enumerate(iterator[:n_books]):
            try:
                info = books[books.title == i]
                '**---**'
                '**Book title:**', info.title.tolist()[0]
                '**Author:**', info.name.tolist()[0]
                '**Weighted Score**', str(round(info.weighted_score.tolist()[0], 2)), '/ 5'

                showReviewClusters = st.button(label='Show opinion clusters for this book?', key=idx)
                showAuthorClusters = st.button(label='Show opinion clusters for this author?', key=idx+100)
            except IndexError:
                break

            if showReviewClusters:
                with clusters:
                    try:
                        input_sentences, input_vectors = embedInputs(books,
                                                                    reviewsAll,
                                                                    search_param=info.title.tolist()[0],
                                                                    review_max_len=review_max_len,
                                                                    searchTitle=True)
                        model = getClusters(input_vectors=input_vectors,
                                            n_clusters=n_clusters)
                        showClusters(input_sentences=input_sentences,
                                    input_vectors=input_vectors,
                                    authorTitle = info.title.tolist()[0],
                                    n_clusters=n_clusters,
                                    n_results=n_results,
                                    model=model,
                                    searchTitle=True)
                    except ValueError:
                        st.warning(f"It looks like this book doesn't have enough reviews to generate {n_clusters} distinct themes. Try decreasing how many themes you look for!")
                        continue
            if showAuthorClusters:
                with clusters:
                    try:
                        input_sentences, input_vectors = embedInputs(books,
                                                                    reviewsAll,
                                                                    search_param=info.name.tolist()[0],
                                                                    review_max_len=review_max_len,
                                                                    searchTitle=False)
                        model = getClusters(input_vectors=input_vectors,
                                            n_clusters=n_clusters)
                        showClusters(input_sentences=input_sentences,
                                    input_vectors=input_vectors,
                                    authorTitle = info.name.tolist()[0],
                                    n_clusters=n_clusters,
                                    n_results=n_results,
                                    model=model,
                                    searchTitle=False)
                    except ValueError:
                        st.warning(f"It looks like this author's books don't have enough reviews to generate {n_clusters} distinct themes. Try decreasing how many themes you look for!")
                        continue
            if goodreadsLink:
                good_reads_link = goodreadsURL + info.book_id.astype(str).tolist()[0]
                st.write(f'*Goodreads Link: {good_reads_link}*')
#######################################################################################
                            # Load variables and data
#######################################################################################

# Paths to books and reviews DataFrames
datapath = '/media/einhard/Seagate Expansion Drive/3380_data/data/Filtered books/'

# Stores tokenized reviews so they only need to be processed the first time that particular book is called
tokenizedData = '/media/einhard/Seagate Expansion Drive/3380_data/data/app_data/'
books_file = 'clean_filtered_books.csv'
reviews_file = 'clean_filtered_reviews.csv'
reviewsAll_file = 'reviews_for_cluster.csv'

# Loading DataFrames
books, reviews, reviewsAll = dataLoader(datapath, books_file, reviews_file, reviewsAll_file)

# Loadding pre-trained embeddings and embedder for input sentences
embed, reviews_array, descriptions_array = loadEmbeddings()

# Setting base URL for Goodreads
goodreadsURL = 'https://www.goodreads.com/book/show/'


#######################################################################################
                                # Web App
#######################################################################################

'''# 3380 Books'''

st.sidebar.markdown(
    '''
    # 3380 Books
    *Read books, not reviews!*
    '''
)

# Creating options dropdown menu in sidebar
options = st.sidebar.beta_expander('Options')
with options:
    goodreadsLink = st.checkbox('Show Goodreads links.')
    n_clusters = st.slider('Select how many themes to search for in the reviews',
                                2,10,value=8,step=1)
    n_results = st.slider('Select how many reviews to show per theme',
                                1,15,value=8,step=1)
    n_books = st.slider('Select how many book results to show',
                                1, 25, value=10, step=1)
    review_max_len = st.slider('Select maximum review length for each theme group',
                                 50, 350, value=350, step=10)



# Asking for user input
input_text = st.text_input('Try specifying `author:` or `title:` for more specific results')

# Creating columns for book results on the left, review clusters on the right
results, clusters = st.beta_columns(2)

# Welcome message
if not input_text:
    '''
    ### How this works:
    In the search bar above, you can type in any sentence that describes a book you'd like to read.

    The machine learning algorithm will then look through a database of books and reviews to find the most appropriate recommendations for you based on how other people review the books they read (to be more specific, it will find the most semantically similar reviews).

    If you want to explore further, you can generate thematically linked clusters for any book. On the sidebar, you can adjust how many opinion themes are generated for a particular book, as well as the number of reviews per theme and a couple of other options.
    '''
    about = st.beta_expander('About')
    with about:
        '''
        ## About
        
        3380 Books was created as part of my final data science project at [Lighthouse Labs, Vancouver](https://www.lighthouselabs.ca/).

        The inspiration for the app came from Goodread's yearly reading [challenge](https://www.goodreads.com/challenges/show/11621-2020-reading-challenge). The name, 3380, comes from a simple calculation: if you were born in 1993, like I was, your [average global life expectancy at birth](https://data.worldbank.org/indicator/SP.DYN.LE00.IN) would be around 65 years. If you committed to reading one book every single week from the day you were born till the day you die, you'd read 52 books per year, or about 3,380 books in your lifetime.

        To put that in perspective,  there were [45,860 working authors](https://www.statista.com/topics/1177/book-market/) in 2019 and over 1.5 million self-published books in 2018 *in the US alone*. As of late 2019, the Google Books project had scanned over [40 million books in over 400 languages](https://www.blog.google/products/search/15-years-google-books/). This means that with 3380 books to read in a lifetime, we will still only be able to read less than 0.01% of extant books (and every year more and more books are being published). It is therefore increasingly important to find a way to filter information; but, crucially, to not rely *only* on other users - in the way that Netflix suggests TV shows, for example. Rather, it
        behooves us to take an active part in choosing the content we want to consume.
        '''

    acknowledgements = st.beta_expander('Acknowledgements')
    with acknowledgements:
        '''
        ## Acknowledgements

        I'd like to start by thanking Menging Wan, Julian McAuley, Rishabh Misra and Ndapa Nakashole for creating and publishing the [UCSD Book Graph](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home)
        database. All the book data for this app comes from their work. If you're interested, check out their papers: [*Item Recommendation on Monotonic Behaviour Chains*](https://github.com/MengtingWan/mengtingwan.github.io/raw/master/paperrecsys18_mwan.pdf), and [*Fine-Grained Spoiler Detection from Large-Scale Review Corpora*](https://www.aclweb.org/anthology/P19-1248/).

        I would also like to thank all the staff at Lighthouse Labs for helping us through this journey.

        Most of all, I'd like to thank my classmates, without whom I surely would not have made it this far. In particular, for their immense support and unlimited camaraderie, I'd like to acknowledge:
        * [Atlas Kazemian](https://www.linkedin.com/in/atlas-kazemian-874b11100/)
        * [Olivia Kim](https://github.com/yjik122)
        * [Elliot Lupini](https://www.linkedin.com/in/elliot-lupini-8824681b1/)
        * [Lane Clark](http://lclark.ca/)
        * [Henri Vandersleyen](https://www.linkedin.com/in/henri-vandersleyen-a25a8312b/)
        '''

# Title specific book searches
elif re.match(r'title: ', input_text):
    input_text = input_text.replace('title: ', '')
    book_title = books[books.title.str.contains(input_text, case=False)].title.tolist()
    showInfo(iterator=book_title,
            n_clusters=n_clusters,
            n_results=n_results,
            n_books=n_books,
            review_max_len=review_max_len)


# Author specific searches
elif re.match(r'author: ', input_text):
    input_text = input_text.replace('author: ', '')
    author_name = books[books.name.str.contains(input_text, case=False)].title.tolist()
    showInfo(iterator=author_name,
            n_clusters=n_clusters,
            n_results=n_results,
            n_books=n_books,
            review_max_len=review_max_len)

# Description specific searches
elif re.match(r'description: ', input_text):
    input_text = input_text.replace('description: ', '')




if psutil.virtual_memory()[2] > 60:
    st.write('Clearing cache to make sure things continue to run smoothly. Hang on!')
    caching.clear_cache()